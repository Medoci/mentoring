{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data from website using scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://cycling.data.tfl.gov.uk/'\n",
    "page = requests.get(URL)\n",
    "\n",
    "#create bs4 object\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_elements = soup.find_all(\"tr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Webpage uses JS to load the table, so we will use selenium first to scrape the content and then BS4 to get links for files - before then using HTTP requests for the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented out - the below code initially saved the flat HTML content in a html file, which is saved in the data folder\n",
    "\n",
    "# driver = webdriver.Edge()\n",
    "# driver.get(URL)\n",
    "# driver.set_window_position(0, 0)\n",
    "# driver.set_window_size(100000, 200000)\n",
    "# driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "# time.sleep(5) # wait to load\n",
    "\n",
    "# # now print the response\n",
    "# #print(driver.page_source)\n",
    "\n",
    "# soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# with open(\"data\\\\tfl_data.html\", \"w\") as file:\n",
    "#     file.write(str(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open html file of the tfl website\n",
    "\n",
    "with open('data\\\\tfl_data.html', 'r') as file:\n",
    "    html_content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to BS4 object for scraping\n",
    "\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get table with all files in\n",
    "\n",
    "table = soup.find(id=\"tbody-content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all URLs from the table for download\n",
    "\n",
    "all_links = []\n",
    "\n",
    "for tr in table.find_all('tr'):\n",
    "    dl = tr.get('data-level')\n",
    "\n",
    "    #only get level 3 links\n",
    "    if dl == '3':\n",
    "        # get a href tag for download link\n",
    "        a_links = tr.find_all('a', href=True)\n",
    "        # only get csv files\n",
    "        if tr.find_all('td')[3].string == \"CSV file\":\n",
    "            # only add link where there is a link that exists\n",
    "            if len(a_links) > 0:\n",
    "                all_links.append(a_links[0]['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the links that are just for the usage stats that are for years 2019-2021\n",
    "\n",
    "usage_links_all = []\n",
    "\n",
    "for l in all_links:\n",
    "    if l[32:43] == 'usage-stats' and (l[-6:-4] == '19' or l[-6:-4] == '20' or l[-6:-4] == '21'):\n",
    "        usage_links_all.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the first url which is data for 2018 to first day of 2019\n",
    "usage_links_all = usage_links_all[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Rental Id  Duration  Bike Id          End Date  EndStation Id  \\\n",
      "0          83284852       660     8282  02/01/2019 17:47             94   \n",
      "1          83360769       180     4657  06/01/2019 18:14             94   \n",
      "2          83280311       960     6306  02/01/2019 14:49            374   \n",
      "3          83323626       120     6217  04/01/2019 12:54            269   \n",
      "4          83343322       120      848  05/01/2019 16:03            269   \n",
      "...             ...       ...      ...               ...            ...   \n",
      "31348497  115876448      1080    19041  28/12/2021 13:14            148   \n",
      "31348498  115859467      1440      203  26/12/2021 16:15              9   \n",
      "31348499  115851371      1260    19576  25/12/2021 18:03            602   \n",
      "31348500  115850742       900     9180  25/12/2021 17:23            794   \n",
      "31348501  115860466       600    17062  26/12/2021 17:24            149   \n",
      "\n",
      "                            EndStation Name        Start Date  \\\n",
      "0                 Bricklayers Arms, Borough  02/01/2019 17:36   \n",
      "1                 Bricklayers Arms, Borough  06/01/2019 18:11   \n",
      "2              Waterloo Station 1, Waterloo  02/01/2019 14:33   \n",
      "3                Empire Square, The Borough  04/01/2019 12:52   \n",
      "4                Empire Square, The Borough  05/01/2019 16:01   \n",
      "...                                     ...               ...   \n",
      "31348497         Tachbrook Street, Victoria  28/12/2021 12:56   \n",
      "31348498           New Globe Walk, Bankside  26/12/2021 15:51   \n",
      "31348499       Union Grove, Wandsworth Road  25/12/2021 17:42   \n",
      "31348500      Victoria Rise, Clapham Common  25/12/2021 17:08   \n",
      "31348501  Kennington Road Post Office, Oval  26/12/2021 17:14   \n",
      "\n",
      "          StartStation Id              StartStation Name  \n",
      "0                     197    Stamford Street, South Bank  \n",
      "1                     269     Empire Square, The Borough  \n",
      "2                       5  Sedding Street, Sloane Square  \n",
      "3                      94      Bricklayers Arms, Borough  \n",
      "4                      94      Bricklayers Arms, Borough  \n",
      "...                   ...                            ...  \n",
      "31348497              603     Caldwell Street, Stockwell  \n",
      "31348498              603     Caldwell Street, Stockwell  \n",
      "31348499              603     Caldwell Street, Stockwell  \n",
      "31348500              603     Caldwell Street, Stockwell  \n",
      "31348501              603     Caldwell Street, Stockwell  \n",
      "\n",
      "[31348502 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# This is all commented to reduce run time.\n",
    "# The below code requested all usage stats data and then saved the data in a csv file\n",
    "\n",
    "\n",
    "# # to allow for csv reading\n",
    "# storage_options = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "# def merge_csv_data(urls):\n",
    "#     dfs = []\n",
    "#     for url in urls:\n",
    "#         # Read CSV data from URL\n",
    "#         df = pd.read_csv(url, storage_options=storage_options)\n",
    "#         # Append dataframe to list\n",
    "#         dfs.append(df)\n",
    "    \n",
    "#     # Concatenate all dataframes in the list into one dataframe\n",
    "#     merged_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "#     return merged_df\n",
    "\n",
    "# # List of URLs pointing to CSV files\n",
    "# urls = usage_links_all\n",
    "\n",
    "# # Call the function and get the merged dataframe\n",
    "# merged_dataframe = merge_csv_data(urls)\n",
    "\n",
    "# merged_dataframe.to_csv('data\\\\all_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data from saved CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data\\\\all_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31348502, 10)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Very large dataset, 31 million rows of data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0            int64\n",
       "Rental Id             int64\n",
       "Duration              int64\n",
       "Bike Id               int64\n",
       "End Date             object\n",
       "EndStation Id         int64\n",
       "EndStation Name      object\n",
       "Start Date           object\n",
       "StartStation Id       int64\n",
       "StartStation Name    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Unnamed: 0'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\SimpsonJ\\OneDrive - Vodafone Group\\Documents\\Ben_Mentoring\\mentoring\\jack-simpson-cycling\\cycling_analysis.ipynb Cell 19\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/SimpsonJ/OneDrive%20-%20Vodafone%20Group/Documents/Ben_Mentoring/mentoring/jack-simpson-cycling/cycling_analysis.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#drop unnamed:0\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/SimpsonJ/OneDrive%20-%20Vodafone%20Group/Documents/Ben_Mentoring/mentoring/jack-simpson-cycling/cycling_analysis.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df\u001b[39m.\u001b[39;49mdrop([\u001b[39m'\u001b[39;49m\u001b[39mUnnamed: 0\u001b[39;49m\u001b[39m'\u001b[39;49m], axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\SimpsonJ\\AppData\\Local\\miniconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5347\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5199\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop\u001b[39m(\n\u001b[0;32m   5200\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   5201\u001b[0m     labels: IndexLabel \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5208\u001b[0m     errors: IgnoreRaise \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   5209\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   5210\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   5211\u001b[0m \u001b[39m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5212\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5345\u001b[0m \u001b[39m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5346\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5347\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdrop(\n\u001b[0;32m   5348\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   5349\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   5350\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m   5351\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   5352\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m   5353\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[0;32m   5354\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   5355\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\SimpsonJ\\AppData\\Local\\miniconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4711\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4709\u001b[0m \u001b[39mfor\u001b[39;00m axis, labels \u001b[39min\u001b[39;00m axes\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   4710\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 4711\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_drop_axis(labels, axis, level\u001b[39m=\u001b[39;49mlevel, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4713\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[0;32m   4714\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\SimpsonJ\\AppData\\Local\\miniconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4753\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4751\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mdrop(labels, level\u001b[39m=\u001b[39mlevel, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m   4752\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 4753\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39;49mdrop(labels, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4754\u001b[0m     indexer \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4756\u001b[0m \u001b[39m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4757\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\SimpsonJ\\AppData\\Local\\miniconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6992\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6990\u001b[0m \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[0;32m   6991\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m-> 6992\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlabels[mask]\u001b[39m.\u001b[39mtolist()\u001b[39m}\u001b[39;00m\u001b[39m not found in axis\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6993\u001b[0m     indexer \u001b[39m=\u001b[39m indexer[\u001b[39m~\u001b[39mmask]\n\u001b[0;32m   6994\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Unnamed: 0'] not found in axis\""
     ]
    }
   ],
   "source": [
    "#drop unnamed:0\n",
    "df.drop(['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert start and end dates to DateTime values\n",
    "df['Start Date'] = pd.to_datetime(df['Start Date'], dayfirst=True)\n",
    "df['End Date'] = pd.to_datetime(df['End Date'], dayfirst=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
