{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data from website using scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://cycling.data.tfl.gov.uk/'\n",
    "page = requests.get(URL)\n",
    "\n",
    "#create bs4 object\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_elements = soup.find_all(\"tr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Webpage uses JS to load the table, so we will use selenium first to scrape the content and then BS4 to get links for files - before then using HTTP requests for the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented out - the below code initially saved the flat HTML content in a html file, which is saved in the data folder\n",
    "\n",
    "# driver = webdriver.Edge()\n",
    "# driver.get(URL)\n",
    "# driver.set_window_position(0, 0)\n",
    "# driver.set_window_size(100000, 200000)\n",
    "# driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "# time.sleep(5) # wait to load\n",
    "\n",
    "# # now print the response\n",
    "# #print(driver.page_source)\n",
    "\n",
    "# soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# with open(\"data\\\\tfl_data.html\", \"w\") as file:\n",
    "#     file.write(str(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open html file of the tfl website\n",
    "\n",
    "with open('data\\\\tfl_data.html', 'r') as file:\n",
    "    html_content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to BS4 object for scraping\n",
    "\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get table with all files in\n",
    "\n",
    "table = soup.find(id=\"tbody-content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all URLs from the table for download\n",
    "\n",
    "all_links = []\n",
    "\n",
    "for tr in table.find_all('tr'):\n",
    "    dl = tr.get('data-level')\n",
    "\n",
    "    #only get level 3 links\n",
    "    if dl == '3':\n",
    "        # get a href tag for download link\n",
    "        a_links = tr.find_all('a', href=True)\n",
    "        # only get csv files\n",
    "        if tr.find_all('td')[3].string == \"CSV file\":\n",
    "            # only add link where there is a link that exists\n",
    "            if len(a_links) > 0:\n",
    "                all_links.append(a_links[0]['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the links that are just for the usage stats that are for years 2019-2021\n",
    "\n",
    "usage_links_all = []\n",
    "\n",
    "for l in all_links:\n",
    "    if l[32:43] == 'usage-stats' and (l[-6:-4] == '19' or l[-6:-4] == '20' or l[-6:-4] == '21'):\n",
    "        usage_links_all.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the first url which is data for 2018 to first day of 2019\n",
    "usage_links_all = usage_links_all[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is all commented to reduce run time.\n",
    "# The below code requested all usage stats data and then saved the data in a csv file\n",
    "\n",
    "\n",
    "# # to allow for csv reading\n",
    "# storage_options = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "# def merge_csv_data(urls):\n",
    "#     dfs = []\n",
    "#     for url in urls:\n",
    "#         # Read CSV data from URL\n",
    "#         df = pd.read_csv(url, storage_options=storage_options)\n",
    "#         # Append dataframe to list\n",
    "#         dfs.append(df)\n",
    "    \n",
    "#     # Concatenate all dataframes in the list into one dataframe\n",
    "#     merged_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "#     return merged_df\n",
    "\n",
    "# # List of URLs pointing to CSV files\n",
    "# urls = usage_links_all\n",
    "\n",
    "# # Call the function and get the merged dataframe\n",
    "# merged_dataframe = merge_csv_data(urls)\n",
    "\n",
    "# merged_dataframe.to_csv('data\\\\all_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data from saved CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data\\\\all_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31348502, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Very large dataset, 31 million rows of data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0            int64\n",
       "Rental Id             int64\n",
       "Duration              int64\n",
       "Bike Id               int64\n",
       "End Date             object\n",
       "EndStation Id         int64\n",
       "EndStation Name      object\n",
       "Start Date           object\n",
       "StartStation Id       int64\n",
       "StartStation Name    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unnamed:0\n",
    "df.drop(['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert start and end dates to DateTime values\n",
    "df['Start Date'] = pd.to_datetime(df['Start Date'], dayfirst=True)\n",
    "df['End Date'] = pd.to_datetime(df['End Date'], dayfirst=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
